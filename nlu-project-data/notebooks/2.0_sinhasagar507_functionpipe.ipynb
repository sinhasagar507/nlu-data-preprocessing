{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0db0df15",
   "metadata": {
    "id": "0db0df15"
   },
   "outputs": [],
   "source": [
    "__author__ = \"Sagar Sinha\"\n",
    "__task__ = \"Text preprocessing and analysis pipeline\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "901723d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (1762639170.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[2], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    from ..\\src\\features\\pipeline import pipeline\u001b[0m\n\u001b[1;37m                                                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "from ..\\src\\features\\pipeline import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96c08b63",
   "metadata": {
    "id": "96c08b63"
   },
   "outputs": [],
   "source": [
    "# Check for version consistency of Corpus \n",
    "try:\n",
    "    from collections import Counter, defaultdict \n",
    "    import re\n",
    "    import os\n",
    "    import json\n",
    "    import string \n",
    "   \n",
    "    # Import third-party libraries\n",
    "    import pandas as pd\n",
    "    \n",
    "    import warnings \n",
    "    warnings.filterwarnings('ignore')\n",
    "    \n",
    "    import numpy as np\n",
    "    from collections import defaultdict\n",
    "    from tqdm import tqdm \n",
    "    from convokit import Corpus, Speaker, Utterance, download\n",
    "    import convokit\n",
    "    from convokit.text_processing import TextProcessor, TextCleaner, TextParser\n",
    "    from convokit.convokitPipeline import ConvokitPipeline\n",
    "    from convokit import PolitenessStrategies\n",
    "    from textblob import TextBlob as tb\n",
    "    import nltk\n",
    "    from nltk.stem import WordNetLemmatizer \n",
    "    import spacy \n",
    "    import emot\n",
    "    import contractions as cm \n",
    "    import vaderSentiment\n",
    "    from googleapiclient import discovery\n",
    "    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "   \n",
    "    \n",
    "    ### Import langdetect and set seed = 0 to enforce consistent results \n",
    "    from langdetect import DetectorFactory, detect\n",
    "    DetectorFactory.seed = 0 \n",
    "    \n",
    "    # The below modules can be implemented after the utterances have been loaded as Pandas Dataframe or Numpy arrays \n",
    "    # import sklearn                    \n",
    "    # from sklearn.preprocessing import FunctionTransformer\n",
    "    # from sklearn.base import BaseEstimator, TransformerMixin\n",
    "    # from sklearn.pipeline import Pipeline, make_pipeline, FeatureUnion, make_union\n",
    "    # from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "\n",
    "except ImportError as e:\n",
    "  print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5085a298",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5085a298",
    "outputId": "4715aefd-f9f9-4687-9ac5-d70e2f17e2f7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK libraries and dependencies for preprocessing\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "FI76U3nowmQE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "FI76U3nowmQE",
    "outputId": "fde88249-88d9-4e5e-f9d8-22182e4918e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-md==3.3.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.3.0/en_core_web_md-3.3.0-py3-none-any.whl (33.5 MB)\n",
      "Requirement already satisfied: spacy<3.4.0,>=3.3.0.dev0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from en-core-web-md==3.3.0) (3.3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (3.1.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (4.62.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (2.26.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (1.21.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (1.0.7)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (58.0.4)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (0.4.1)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (0.6.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (1.0.2)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (8.0.16)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (3.3.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (3.0.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (21.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (2.0.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (2.0.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (3.0.9)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (0.7.7)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (1.8.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (0.9.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (2.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (3.10.0.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (2022.9.24)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (2.1.1)\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "# Convokit requires a SpaCy model to be installed. Run `python -m spacy download MODEL_NAME`\n",
    "! python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "vh5S6vEKwJAD",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vh5S6vEKwJAD",
    "outputId": "378d515e-41e0-4560-f406-4cc56d838b53"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x24f96c984f0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the spacy language model \n",
    "# A custom language model can be loaded. Check \n",
    "spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "729c7d66",
   "metadata": {
    "id": "729c7d66"
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer() # Installing WordNet Lemmatizer \n",
    "sentiment =  SentimentIntensityAnalyzer()  # Intialize Vader Sentiment Analyzer \n",
    "emotion = emot.core.emot()  # Initialize Emoji Object\n",
    "politeness = PolitenessStrategies() # Politeness Indicators\n",
    "spacy_nlp = spacy.load('en_core_web_md', disable=['ner']) # Spacy Language Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "faf46d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is OS\n",
      " Volume Serial Number is 6809-FEE9\n",
      "\n",
      " Directory of C:\\Sagar Study\\ML and Learning\\CP Sem-8\\Data\\Reddit\\saved-corpora\n",
      "\n",
      "19-11-2022  08:53    <DIR>          .\n",
      "19-11-2022  08:53    <DIR>          ..\n",
      "21-10-2022  22:49    <DIR>          .ipynb_checkpoints\n",
      "19-11-2022  08:52            36,489 1.0_conversation_metadata .ipynb\n",
      "17-08-2022  18:39            37,440 1_0_Adding_Metadata.ipynb\n",
      "19-11-2022  08:53            50,888 2_0_function_migration_pipeline.ipynb\n",
      "13-08-2022  10:25           269,413 bot-removal.ipynb\n",
      "19-11-2022  08:49    <DIR>          fc(12-23)_27p_1\n",
      "26-08-2022  07:39    <DIR>          lexicons\n",
      "05-08-2022  13:28    <DIR>          mens-rights-2018\n",
      "06-08-2022  14:56    <DIR>          mens-rights-2018-transformed\n",
      "06-08-2022  18:16    <DIR>          mens-rights-2018-transformed-2\n",
      "09-08-2022  20:20    <DIR>          mens-rights-2018-transformed-3\n",
      "09-08-2022  21:02    <DIR>          mens-rights-2018-transformed-final\n",
      "09-08-2022  20:13    <DIR>          transformed-reddit-corpus-2018-3\n",
      "               4 File(s)        394,230 bytes\n",
      "              11 Dir(s)  205,646,143,488 bytes free\n"
     ]
    }
   ],
   "source": [
    "!dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80005205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7cb6bb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "    subjectivity_clues = []\n",
    "\n",
    "    with open(\"lexicons/subjectivity_clues.txt\", \"r+\") as file: \n",
    "        for line in file.readlines():\n",
    "            values = line.split(\" \")[2]\n",
    "            subjectivity_clue = values.split(\"=\")[1]\n",
    "            subjectivity_clues.append(subjectivity_clue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "981a9c9e",
   "metadata": {
    "id": "981a9c9e"
   },
   "outputs": [],
   "source": [
    "# Load the Reddit Corpus \n",
    "try:\n",
    "    CORPUS_PATH = \"sanskar_transformed_corpora/tr_fc(12-23)_27p_1\"   # Provide the path \n",
    "    reddit_corpus = Corpus(CORPUS_PATH)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "431a3309",
   "metadata": {},
   "outputs": [],
   "source": [
    "def def_value():\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35c653a7",
   "metadata": {
    "id": "35c653a7"
   },
   "outputs": [],
   "source": [
    "# Below-mentioned functions would be chained into a universal pipeline for a combined analysis \n",
    "# Only input utterance level tests. Speaker-level and conversation-level tests will be added later as we load the dataframe \n",
    "# First load the raw corpus into Communalytic. Get scores from that platform \n",
    "\n",
    "def extractHashtags(utterance: str) -> list:\n",
    "    \"\"\" Returns all Twitter hashtags from a tweet\n",
    "\n",
    "    Store all hashtags from a tweet and store them in a separate column\n",
    "    \"\"\"\n",
    "    hashtags_ls = re.findall(\"#\\w+\", utterance)\n",
    "    return hashtags_ls\n",
    " \n",
    "hashtags = TextProcessor(proc_fn=extractHashtags, output_field=\"hashtags\")  # We pass the required function as a parameter to TextProcessor class. The output would be stored in metadata as \"hashtags\"\n",
    "\n",
    "def extractUsernameTags(utterance: str) -> list:\n",
    "    \"\"\" Returns all Username tags from a tweet\n",
    "    Store all tags from a tweet and store them in a separate column\n",
    "    \"\"\"\n",
    "\n",
    "    username_tags = re.findall(\"@\\w+\", utterance)\n",
    "    return username_tags\n",
    "\n",
    "reply_to_usernames = TextProcessor(proc_fn=extractUsernameTags, output_field=\"username_tags\") # Output is tsored in metadata as \"username_tags\"\n",
    "\n",
    "\n",
    "def cleanReddit(\n",
    "        text_reddit, newline=True, quote=True,\n",
    "        bullet_point=True, dates=True, link=True,\n",
    "        strikethrough=True, spoiler=True, heading=True,\n",
    "        emoj=True, emoticon=True, condensed=True):\n",
    "    # Newlines we don't need - only\n",
    "    \"\"\" Cleans reddit utterances\"\"\"\n",
    "    \n",
    "    if newline:\n",
    "        text_reddit = re.sub(r'\\n+', ' ', text_reddit)\n",
    "        # Remove the many \" \" that we replaced in the last step\n",
    "        text_reddit = text_reddit.strip()\n",
    "        text_reddit = re.sub(r'\\s\\s+', ' ', text_reddit)\n",
    "\n",
    "    # > are for the quoted texts from the main comment or the reply\n",
    "    if quote:\n",
    "        text_reddit = re.sub(r'>', '', text_reddit)\n",
    "\n",
    "    # Bullet points/asterisk are used for markdown like - bold/italic - Could create trouble in parsing? idk\n",
    "    if bullet_point:\n",
    "        text_reddit = re.sub(r'\\*', '', text_reddit)\n",
    "        text_reddit = re.sub('&amp;#x200B;', '', text_reddit)\n",
    "\n",
    "    # []() Link format then we remove both the tag/placeholder and the link\n",
    "    if link:\n",
    "        text_reddit = re.sub(r\"http\\S+\", '', text_reddit)\n",
    "        text_reddit = re.sub(r'\\[.*?\\]\\(.*?\\)', '', text_reddit)\n",
    "\n",
    "    # Strikethrough\n",
    "    if strikethrough:\n",
    "        text_reddit = re.sub('~', '', text_reddit)\n",
    "\n",
    "    # Spoiler, which is used with < less-than (Preserves the text)\n",
    "    if spoiler:\n",
    "        text_reddit = re.sub('&lt;', '', text_reddit)\n",
    "        text_reddit = re.sub(r'!(.*?)!', r'\\1', text_reddit)\n",
    "\n",
    "    # Heading to be removed as there are these markdown style features in reddit too\n",
    "    if heading:\n",
    "        text_reddit = re.sub('#', '', text_reddit)\n",
    "\n",
    "    if emoj:\n",
    "        # Implement the emoji scheme here\n",
    "        # Implementing a Naive Emoji Scheme\n",
    "        # Some associated libraries are EMOT and DEMOJI\n",
    "        # text_reddit = emoji.demojize(text_reddit).replace(\":\", \"\").replace(\"_\", \"\")\n",
    "        # Makes more sense for the node feature but might as well import that function here if ready\n",
    "        pass\n",
    "\n",
    "    if dates:\n",
    "        text_reddit = re.sub(r'(\\d+/\\d+/\\d+)', '', text_reddit)\n",
    "\n",
    "    if emoticon:\n",
    "        # Implement the emoticon scheme here.\n",
    "        # Makes more sense for the node feature but might as well import that function here if ready\n",
    "        pass\n",
    "\n",
    "    # Needs to be the last step in the process\n",
    "    # if contractions:\n",
    "    # text = contractions.fix(text)\n",
    "    # print(\"Running\")\n",
    "    return text_reddit\n",
    "     \n",
    "clean_text_reddit = TextCleaner(text_cleaner = cleanReddit, replace_text = False, save_original = True)\n",
    "    \n",
    "def cleanTwitter(\n",
    "        text_twitter, urls=True, tags=True,\n",
    "        newLine=True, ellipsis=True, ampersand=True,\n",
    "        tilde=True, special_chars=True, dollar=True,\n",
    "        commas_semicols=True, bracketed_phrases=True, contractions=True,\n",
    "        quotation_marks=True, greater_than_less_than=True, question_mark_exclaim=True,\n",
    "        character_encodings=True, trademark=True, condensed=True) -> str:\n",
    "    \"\"\" Clean tweets after extracting all hashtags and username tags\n",
    "    Not comprehensive enough to capture all idiosyncrasies, but works for most of the time\n",
    "    \"\"\"\n",
    "    if urls:\n",
    "        url_pattern = \"https?:\\/\\/(www\\.)?(\\w+)(\\.\\w+)\\/\\w*\"\n",
    "        text_twitter = re.sub(url_pattern, \"\", text_twitter)\n",
    "\n",
    "    if tags:\n",
    "        text_twitter = re.sub(\"@\\w+\", \"\", text_twitter)\n",
    "\n",
    "    # Remove \"\\n\". One or more occurrences\n",
    "    if newLine:\n",
    "        # Replacing single occurrences of '\\n' with ''\n",
    "        # Replacing multiple occurrences, i.e., >=2 occurrences with '.'\n",
    "        text_twitter = re.sub(\"\\n\", \"\", text_twitter)\n",
    "        text_twitter = re.sub(\"\\n\\n\", \".\", text_twitter)\n",
    "        text_twitter = text_twitter.strip()\n",
    "\n",
    "    # Fix contractions\n",
    "    if condensed:\n",
    "        text_twitter = cm.fix(text_twitter)\n",
    "        text_twitter = re.sub(\"\\s\\s\", \"\", text_twitter)\n",
    "\n",
    "    # Remove \"ellipsis\"\n",
    "    if ellipsis:\n",
    "        text_twitter = re.sub(\"\\.{2,}\", \"\", text_twitter)\n",
    "\n",
    "    # Replace \"&\" with \"and\"\n",
    "    if ampersand:\n",
    "        text_twitter = text_twitter.replace(\"&amp\", \"\")\n",
    "\n",
    "    # Replace \"~\" with \"about\"\n",
    "    if tilde:\n",
    "        text_twitter = re.sub(\"~\", \"about\", text_twitter)\n",
    "\n",
    "    # Remove the special_chars list: [%, ^, *, -, _, +, =, |, \\, /, ?]\n",
    "    if special_chars:\n",
    "        spec_char_list = ['%', '^', '*', '-', '_', '+', '=', '|', '/', '?']\n",
    "        sent = \"\"\n",
    "        new_sent_tokens = []\n",
    "\n",
    "        for character in text_twitter:\n",
    "            if str(character) not in spec_char_list:\n",
    "                new_sent_tokens.append(character)\n",
    "\n",
    "        sent = sent.join(new_sent_tokens)\n",
    "        sent = sent.strip()\n",
    "        text_twitter = sent\n",
    "\n",
    "    # Rename $ as dollar\n",
    "    # if dollar:\n",
    "    #     text_twitter = re.sub(\"$\", \"dollar\", text_twitter)\n",
    "\n",
    "    # Remove brackets and any text enclosed within simple brackets, usually used for acronyms\n",
    "    if bracketed_phrases:\n",
    "        text_twitter = re.sub(\"\\(\\w+\\)\", \"\", text_twitter)\n",
    "\n",
    "    # If single quotes or double quotes have been used in tweets, encash their meaning for the time being. Don't include any other information\n",
    "    if quotation_marks:\n",
    "        text_twitter = re.sub(\"(\\'|\\\")[a-zA-Z0-9\\s+\\.]*(\\'|\\\")\", \"\", text_twitter)\n",
    "\n",
    "    # For the time being, replace commas by \"\" and semicolons by \".\"\n",
    "    if commas_semicols:\n",
    "        text_twitter = re.sub(\"\\,+\", \"\", text_twitter)\n",
    "        text_twitter = re.sub(\"\\;+\", \".\", text_twitter)\n",
    "\n",
    "    # Resolve '>' and '<'\n",
    "    # Replace these characters with their respective names\n",
    "#     if greater_than_less_than:\n",
    "#         text_twitter = re.sub(\"<\", \"is less than\", text_twitter)\n",
    "#         text_twitter = re.sub(\">\", \"is greater than\", text_twitter)\n",
    "#         text_twitter = re.sub(\"<=\", \"is less than or equal to\", text_twitter)\n",
    "#         text_twitter = re.sub(\">=\", \"is greater than or equal to\", text_twitter)\n",
    "\n",
    "    # For the time being, replace and interjections with a full stop\n",
    "    if question_mark_exclaim:\n",
    "        text_twitter = re.sub(\"(\\?|\\!)+\", \".\", text_twitter)\n",
    "\n",
    "    # Resolve character encodings\n",
    "    if character_encodings:\n",
    "        text_twitter = re.sub(\"â|€|¦|â|€˜|€™\", \"\", text_twitter)\n",
    "\n",
    "    # Remove trademark symbol\n",
    "    if trademark:\n",
    "        text_twitter = re.sub(\"\\u2122\", \"\", text_twitter)\n",
    "\n",
    "    return text_twitter\n",
    "\n",
    "clean_text_twitter = TextCleaner(text_cleaner = cleanTwitter, replace_text = False, save_original = True)\n",
    "\n",
    "def convert_to_lower(utt: str) -> str:\n",
    "    \"\"\" This function block performs twitter text normalization\n",
    "\n",
    "        For instance, the different forms of 'hate' are: Hate, HATE, haTE, etc. This function would convert all such occurences to a single canonical form\n",
    "        \"\"\"\n",
    "    exclude_tags_list = ['NN', 'NNS', 'NNP', 'NNPS']  # Check if the attached POS tags are correct or not\n",
    "    sents = nltk.sent_tokenize(utt)\n",
    "    modified_sent_ls = []\n",
    "\n",
    "    for sent in sents: \n",
    "        modified_token_ls = []\n",
    "        words = nltk.word_tokenize(sent)  # Tokenize the sentence and extract POS tags\n",
    "\n",
    "        words = [lemmatizer.lemmatize(word) for word in words]  # Perform lemmatization if required\n",
    "        word_pos_tags = nltk.pos_tag(words)\n",
    "\n",
    "        for (word, tag) in word_pos_tags:\n",
    "            if tag not in exclude_tags_list or word != \"I\":\n",
    "                word = word.lower()\n",
    "                modified_token_ls.append(word)\n",
    "                \n",
    "        modified_token_ls[0] = modified_token_ls[0].capitalize()\n",
    "                \n",
    "        utt = \" \".join(modified_token_ls)\n",
    "        utt = utt.strip()\n",
    "        modified_sent_ls.append(sent)\n",
    "        \n",
    "    final_text = \" \".join(modified_sent_ls)\n",
    "    \n",
    "    return final_text\n",
    "\n",
    "lowercase_text = TextProcessor(proc_fn = convert_to_lower, output_field = \"lowercase_text\")\n",
    "\n",
    "# Use a combination of IDENTITY ATTACK and INSULT parameters to separate MICROAGGRESSIONS from OTHER HATE-SPEECH forms \n",
    "def sentiment_analyzer(utt: str) -> dict:\n",
    "    sentence_ls = nltk.sent_tokenize(utt)\n",
    "    sentiment_score_ls = []\n",
    "\n",
    "    for sent in sentence_ls:\n",
    "        sentiment_score_ls.append(sentiment.polarity_scores(sent))\n",
    "\n",
    "    pos_score_sum, neu_score_sum, neg_score_sum = 0, 0, 0\n",
    "    for sentiment_scores in sentiment_score_ls:\n",
    "        pos_score_sum += sentiment_scores[\"pos\"]\n",
    "        neu_score_sum += sentiment_scores[\"neu\"]\n",
    "        neg_score_sum += sentiment_scores[\"neg\"]\n",
    "\n",
    "    pos_score_sum_avg = round((pos_score_sum / len(sentiment_score_ls)), 3)\n",
    "    neu_score_sum_avg = round((neu_score_sum / len(sentiment_score_ls)), 3)\n",
    "    neg_score_sum_avg = round((neg_score_sum / len(sentiment_score_ls)), 3)\n",
    "\n",
    "    compound_sentiment_scores = {\n",
    "        \"pos\": pos_score_sum_avg,\n",
    "        \"neu\": neu_score_sum_avg,\n",
    "        \"neg\": neg_score_sum_avg\n",
    "    }\n",
    "    return compound_sentiment_scores\n",
    "    \n",
    "analyze_sentiment = TextProcessor(proc_fn = sentiment_analyzer, input_field=\"lowercase_text\",\n",
    "                                  output_field=\"sentiment_polarity\")\n",
    "\n",
    "def modifier_count(utt: str) -> int:   # Calculating less of something isn't always the best indicator. Instead the prevalence of something more than ususal is a better marker. # Optional - Emergency Toolkit  \n",
    "    \"\"\"Count modifiers, i.e., adjectives and adverbs in an utterance\n",
    "    Practically every sentence has modifiers. This function doesn't act as a filter. It is intended to be applied to the entire dataframe\n",
    "    The function block can detect probable deceptive clues in tweets and reddit posts\n",
    "    Less usage of descriptive modifiers is a possible clue that the speaker is uncertain in his claims/opinions.  \n",
    "    This function is more of an emergency toolkit \n",
    "    \"\"\"\n",
    "    \n",
    "    adj_pos_tags = ['JJ', 'JJR', 'JJS']  # POS tags describing adjectives\n",
    "    adv_pos_tags = ['RB', 'RBR', 'RBS']  # POS tags for adverbs\n",
    "    words = nltk.word_tokenize(utt)\n",
    "    word_tag_lst = nltk.pos_tag(words)\n",
    "    mod_count_dict = defaultdict(def_value)\n",
    "    count_mod_tags = 0\n",
    "    for (word, tag) in word_tag_lst:\n",
    "        if tag in adj_pos_tags or tag in adv_pos_tags:\n",
    "            count_mod_tags += 1\n",
    "            mod_count_dict[word] += 1\n",
    "    return {\"modifier_count_dict\": mod_count_dict, \"count_mod_tags\": count_mod_tags}\n",
    "\n",
    "modifier_count = TextProcessor(proc_fn = modifier_count, input_field = \"lowercase_text\", output_field = \"modifier_count\")\n",
    "\n",
    "def hedge_count(utt: str) -> int:\n",
    "    \"\"\" Count the list of all modal verbs that indicate possibility, but not certainty\n",
    "    The function block can detect probable deceptive clues in tweets and reddit posts\n",
    "    More usage of uncertain modal verbs is a possible clue that the speaker is uncertain in his utterance\n",
    "    \"\"\"\n",
    "    count_hedges, hedge_count_dict = 0, defaultdict(def_value)\n",
    "    set_of_hedges_en = [\"almost\", \"apparent\", \"apparently\", \"appear\", \"appeared\", \"appears\", \"approximately\", \"argue\",\n",
    "                        \"argued\", \"argues\", \"around\", \"assume\", \"assumed\", \"broadly\", \"certain amount\",\n",
    "                        \"certain extent\", \"certain level\", \"claim\", \"claimed\", \"claims\", \"doubt\", \"doubtful\",\n",
    "                        \"essentially\", \"estimate\", \"estimated\", \"fairly\", \"feel\", \"feels\", \"felt\", \"frequently\",\n",
    "                        \"from my perspective\",\n",
    "                        \"from our perspective\", \"from this perspective\", \"generally\", \"guess\", \"in general\",\n",
    "                        \"in most cases\", \"in most instances\", \"in my opinion\", \"in my view\", \"in our opinion\",\n",
    "                        \"in our view\",\n",
    "                        \"in this view\", \"indicate\", \"indicated\", \"indicates\", \"largely\", \"likely\", \"mainly\", \"may\",\n",
    "                        \"maybe\", \"might\", \"mostly\", \"often\", \"on the whole\", \"ought\", \"perhaps\", \"plausible\",\n",
    "                        \"plausibly\", \"possible\",\n",
    "                        \"possibly\", \"postulate\", \"postulated\", \"postulates\", \"presumable\", \"presumably\", \"probable\",\n",
    "                        \"probably\", \"quite\", \"rather\", \"relatively\", \"roughly\", \"seems\", \"should\", \"sometimes\",\n",
    "                        \"somewhat\", \"suggest\",\n",
    "                        \"suggested\", \"suggests\", \"suppose\", \"supposed\", \"supposes\", \"suspect\", \"suspects\", \"tend to\",\n",
    "                        \"tended to\", \"tends to\", \"think\", \"thinking\", \"thought\", \"to my knowledge\", \"typical\",\n",
    "                        \"typically\", \"uncertain\",\n",
    "                        \"uncertainly\", \"unclear\", \"unclearly\", \"unlikely\",\n",
    "                        \"usually\"]  # The Hedge word list has been taken from \"https://github.com/tslmy/politeness-estimator.git\"\n",
    "\n",
    "    pos_modal_ls = [\"shall\", \"should\", \"can\", \"could\", \"will\", \"would\", \"may\", \"must\",\n",
    "                    \"might\"]  # List of 9 modal verbs indicating possibility\n",
    "\n",
    "    hedges_modals = set_of_hedges_en + pos_modal_ls\n",
    "\n",
    "    words = utt.lower().split(\" \")\n",
    "    for word in words:\n",
    "        if word in hedges_modals:\n",
    "            count_hedges += 1\n",
    "            hedge_count_dict[word] += 1\n",
    "    return {\"hedge_count_dict\": hedge_count_dict, \"count_hedges\": count_hedges}\n",
    "\n",
    "hedge_modals = TextProcessor(proc_fn = hedge_count, input_field=\"lowercase_text\",\n",
    "                             output_field=\"hedge_count\")\n",
    "\n",
    "def group_ref_count(utt: str) -> int: # Include all third-party pronouns as well\n",
    "    \"\"\"Count list of group references\n",
    "    Usage of more self-references along with subjectivity score is a possible indication of deception \n",
    "    \"\"\"\n",
    "    count_group_ref, group_ref_count_dict = 0, defaultdict(def_value) \n",
    "    words = utt.lower().split()\n",
    "    group_ref = [\"we\", \"our\", \"ours\", \"ourselves\", \"us\", \"they\", \"them\", \"thesmselves\", \"their\", \"theirs\",\n",
    "                 \"everyone\", \"everybody\"]  # More of it to be included here. Self-referencing pronouns\n",
    "\n",
    "    for word in words:\n",
    "        if word in group_ref:\n",
    "            count_group_ref += 1\n",
    "            group_ref_count_dict[word] += 1\n",
    "    return {\"group_ref_count_dict\": group_ref_count_dict, \"count_group_ref\": count_group_ref}\n",
    "\n",
    "group_ref_count = TextProcessor(proc_fn = group_ref_count, input_field = \"lowercase_text\", output_field = \"groupRef_count\")\n",
    "\n",
    "def subjectivity_utterance(utterance: str) -> int:\n",
    "    \"\"\" Textblob subjectivity score\n",
    "    A higher subjective score indicates personal opinion.\n",
    "    Low subjective scores could be a possible indicator of deception. To be used along with self references. \n",
    "    \"\"\"\n",
    "    subjective_lexicon_count, subjective_lexicon_dict, subjectivity_scores = 0, defaultdict(def_value), []\n",
    "    sents = nltk.sent_tokenize(utterance)\n",
    "    \n",
    "    for sent in sents: \n",
    "        subjectivity_scores.append(tb(sent).sentiment.subjectivity)\n",
    "        words = sent.lower().split()\n",
    "        \n",
    "        for word in words: \n",
    "            if word in subjectivity_clues:\n",
    "                subjective_lexicon_dict[word] += 1 \n",
    "                subjective_lexicon_count += 1 \n",
    "            \n",
    "    avg_subjectivity_score = round((sum(subjectivity_scores) / len(subjectivity_scores)), 3)\n",
    "    subjective_details = {\n",
    "                            \"avg_subjectivity_score\": avg_subjectivity_score,\n",
    "                            \"subjective_lexicon_count\": subjective_lexicon_count, \n",
    "                            \"subjective_lexicon_dict\": subjective_lexicon_dict\n",
    "                         }\n",
    "    \n",
    "    return subjective_details\n",
    "      \n",
    "subjectivity = TextProcessor(proc_fn = subjectivity_utterance, input_field = \"lowercase_text\", output_field = \"subjectivity_score\")\n",
    "\n",
    "def count_emojis(utterance: str) -> int: # Optional - Emergency Toolkit - \n",
    "    \"\"\" Counts the total number of emojis in an utterance\n",
    "    We don't intend to delete tweets that have emojis. This function doesn't act as a filter. It is intended to be applied to the entire dataframe\n",
    "    Decide if it is redundant or not - maybe some possible indicators - not the first choice anyhow \n",
    "    \"\"\"\n",
    "    emot_dict = emotion.emoji(utterance)\n",
    "\n",
    "    return len(emot_dict['value'])\n",
    "\n",
    "emoji_count = TextProcessor(proc_fn = count_emojis, input_field = \"lowercase_text\", output_field = \"emoji_count\")\n",
    "\n",
    "def measurePoliteness(utt: str):\n",
    "  \"\"\" Computes politeness indicators in the text. The 9 positive politeness strategies \n",
    "  \"\"\"\n",
    "  # I don't need to split up the utterances here as I am getting an aggregated count of politeness indicators anyway. \n",
    "    \n",
    "  transformed_utt = politeness.transform_utterance(utt, spacy_nlp=spacy_nlp)\n",
    "  return transformed_utt.meta['politeness_strategies']\n",
    "\n",
    "politeness_markers = TextProcessor(proc_fn = measurePoliteness, input_field = \"lowercase_text\", output_field = \"politeness_markers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1d37c9ce",
   "metadata": {
    "id": "1d37c9ce"
   },
   "outputs": [],
   "source": [
    "# The following pipe would be expanded further \n",
    "# Check if all functions from above have been added \n",
    "pipe_reddit = ConvokitPipeline ([\n",
    "                    # (\"clean reddit text\", clean_text_reddit), - To be done later  \n",
    "                    (\"convert to lowercase\", lowercase_text), \n",
    "                    (\"sentiment analyzer\", analyze_sentiment), \n",
    "                    (\"count modifiers\", modifier_count), \n",
    "                    (\"count hedges and modals\", hedge_modals), \n",
    "                    (\"count group references\", group_ref_count),\n",
    "                    (\"subjectivity score\", subjectivity),\n",
    "                    (\"politeness indicators\", politeness_markers)\n",
    "])\n",
    "\n",
    "# The following pipe would be expanded further \n",
    "# pipe_twitter = ConvokitPipeline ([\n",
    "#                     (\"extract hashtags\", hashtags), \n",
    "#                     (\"extract reply_to usernames\", reply_to_usernames),\n",
    "#                     (\"clean twitter text\", clean_text_twitter), \n",
    "#                     (\"convert to lowercase\", lowercase_text), \n",
    "#                     (\"sentiment analyzer\", sentiment_analyzer),\n",
    "#                     (\"count modifiers\", modifier_count), - Emergency Toolkit \n",
    "#                     (\"count hedges and modals\", hedge_modals), \n",
    "#                     (\"count self references\", self_ref_count), \n",
    "#                     (\"subjectivity score\", subjectivity_score),     # Desception - to be used with Self-References and\n",
    "#                     (\"politeness indicators\", politeness_markers)   # Politeness\n",
    "\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d8a3cc08",
   "metadata": {
    "id": "d8a3cc08",
    "outputId": "cc4116a1-2494-45ec-9514-4213051ac7c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congratulations, the reddit corpus has been transformed\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Transform the reddit corpus \n",
    "    transformed_corpus_reddit = pipe_reddit.transform(reddit_corpus, verbosity = 50)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "else:\n",
    "    print(\"Congratulations, the reddit corpus has been transformed\")\n",
    "    \n",
    "# try:\n",
    "#     # Transform the twitter corpus \n",
    "#     transformed_twitter_corpus = pipe_twitter.transform(corpus)\n",
    "# except:\n",
    "#     print(\"No defined corpus\")\n",
    "# else:\n",
    "#     print(\"Congratulations, the twitter corpus has been transformed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60793ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_mod_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec61057",
   "metadata": {
    "id": "0ec61057"
   },
   "source": [
    "## Dump the corpus "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f878c0",
   "metadata": {
    "id": "93f878c0"
   },
   "source": [
    "By default, calling **corpus.dump** will write all of these attributes to disk, within the file that stores utterances; later calling **corpus.load** will load all of these attributes back into a new corpus. For big objects like parses, this incurs a high computational burden (especially if in a later use case you might not even need to look at parses).\n",
    "\n",
    "To avoid this, **corpus.dump** takes an optional argument **fields_to_skip**, which is a **dict of object type ('utterance', 'conversation', 'speaker', 'corpus')** to a list of fields that we do not want to write to disk.\n",
    "\n",
    "The following call will write the corpus to disk, without any of the preprocessing output we generated above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "173b6929",
   "metadata": {
    "id": "173b6929",
    "outputId": "d83ee9c5-98c3-4210-e70a-3fa2f0fbfda5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO OUTPUT DIRECTORY\n"
     ]
    }
   ],
   "source": [
    "# Demonstration of re-dumping of the transformed corpus after first batch of test\n",
    "try:\n",
    "    corpus.dump(os.path.basename(OUT_DIR), base_path=os.path.dirname(OUT_DIR), \n",
    "            fields_to_skip={'utterance': ['parsed','tagged','clean_text']})    # Just random field names \n",
    "    \n",
    "except:\n",
    "    print(\"NO OUTPUT DIRECTORY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d749f9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration of re-dumping of the transformed corpus after first batch of test\n",
    "try:\n",
    "    CORPUS_NAME = \"tr_fc(12-23)_27p_1\"\n",
    "    OUTPUT_DIR = \"C:\\Sagar Study\\ML and Learning\\CP Sem-8\\Data\\Reddit\\saved-corpora\\sanskar_transformed_corpora\"\n",
    "    transformed_corpus_reddit.dump(CORPUS_NAME, base_path=OUTPUT_DIR)\n",
    "    \n",
    "except:\n",
    "    print(\"NO OUTPUT DIRECTORY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e6ea4dc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Utterance({'obj_type': 'utterance', 'meta': {'lowercase_text': \"I mean you're certainly a mega douche for what it's worth\", 'sentiment_polarity': {'pos': 0.291, 'neu': 0.541, 'neg': 0.169}, 'modifier_count': {'modifier_count_dict': defaultdict(<function def_value at 0x0000024FCABB7670>, {'certainly': 1, 'mega': 1, 'worth': 1}), 'count_mod_tags': 3}, 'hedge_count': {'hedge_count_dict': defaultdict(<function def_value at 0x0000024FCABB7670>, {}), 'count_hedges': 0}, 'groupRef_count': {'group_ref_count_dict': defaultdict(<function def_value at 0x0000024FCABB7670>, {}), 'count_group_ref': 0}, 'subjectivity_score': {'avg_subjectivity_score': 0.453, 'subjective_lexicon_count': 3, 'subjective_lexicon_dict': defaultdict(<function def_value at 0x0000024FCABB7670>, {'mean': 1, 'certainly': 1, 'worth': 1})}, 'politeness_markers': {'feature_politeness_==Please==': 0, 'feature_politeness_==Please_start==': 0, 'feature_politeness_==HASHEDGE==': 0, 'feature_politeness_==Indirect_(btw)==': 0, 'feature_politeness_==Hedges==': 0, 'feature_politeness_==Factuality==': 0, 'feature_politeness_==Deference==': 0, 'feature_politeness_==Gratitude==': 0, 'feature_politeness_==Apologizing==': 0, 'feature_politeness_==1st_person_pl.==': 0, 'feature_politeness_==1st_person==': 0, 'feature_politeness_==1st_person_start==': 1, 'feature_politeness_==2nd_person==': 1, 'feature_politeness_==2nd_person_start==': 0, 'feature_politeness_==Indirect_(greeting)==': 0, 'feature_politeness_==Direct_question==': 0, 'feature_politeness_==Direct_start==': 0, 'feature_politeness_==HASPOSITIVE==': 1, 'feature_politeness_==HASNEGATIVE==': 0, 'feature_politeness_==SUBJUNCTIVE==': 0, 'feature_politeness_==INDICATIVE==': 0}, 'toxicity': 0.5566829, 'severe_toxicity': 0.02439716, 'profanity': 0.39409557, 'insult': 0.5678696, 'identity_attack': 0.026408968}, 'vectors': [], 'speaker': Speaker({'obj_type': 'speaker', 'meta': {}, 'vectors': [], 'owner': <convokit.model.corpus.Corpus object at 0x0000024FBFE50DF0>, 'id': 'FitProduce1'}), 'conversation_id': 'iuqrfp', 'reply_to': 'g5mfs49', 'timestamp': 1600454494, 'text': \"I mean you're certainly a mega douche for what it's worth\", 'owner': <convokit.model.corpus.Corpus object at 0x0000024FBFE50DF0>, 'id': 'g5q9jnv'})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " transformed_corpus_reddit.random_utterance()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39b5bc2",
   "metadata": {
    "id": "e39b5bc2"
   },
   "source": [
    "For attributes we want to keep around, but that we don't want to read and write to disk in a big batch with all the other corpus data, corpus.dump_info will dump fields of a Corpus object into separate files. This takes the following arguments as input:\n",
    "\n",
    "* obj_type: which type of Corpus object you're dealing with\n",
    "* fields: a list of the fields to write\n",
    "* dir_name: which directory to write to; by default will write to the directory you read the corpus from\n",
    "\n",
    "This function will write each field in fields to a separate file called **info.<field>.jsonl** where each line of the file is a json-serialized dict: {\"id\": <ID of object>, \"value\": <object.get_info(field)>}."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "2.0_function_migration_pipeline.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
