{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0db0df15",
   "metadata": {
    "id": "0db0df15"
   },
   "outputs": [],
   "source": [
    "__author__ = \"Sagar Sinha\"\n",
    "__task__ = \"Text preprocessing and analysis pipeline\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1da64bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Sagar Study\\ML and Learning\\CP Sem-8\\Project-2-Overall\\nlu-project-prerocessing\\nlu-project-data\\data\\interim\\pre-annotation\\convokit_corpora\\untransformed\n"
     ]
    }
   ],
   "source": [
    "%cd C:\\Sagar Study\\ML and Learning\\CP Sem-8\\Project-2-Overall\\nlu-project-prerocessing\\nlu-project-data\\data\\interim\\pre-annotation\\convokit_corpora\\untransformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a98404c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n%env \\n%run\\n%load\\n%save\\n%history\\n%%writefile.py\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Magic Commands \n",
    "'''\n",
    "%env \n",
    "%run\n",
    "%load\n",
    "%save\n",
    "%history\n",
    "%%writefile.py\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bd41f1d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'vaderSentiment'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mvaderSentiment\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'vaderSentiment'"
     ]
    }
   ],
   "source": [
    "import vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1209dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No module named 'vaderSentiment'\n"
     ]
    }
   ],
   "source": [
    "# %load C:\\Sagar Study\\ML and Learning\\CP Sem-8\\Project-2-Overall\\nlu-project-prerocessing\\nlu-project-data\\src\\features\\heuristic_features.py\n",
    "try:\n",
    "    import re\n",
    "    import os\n",
    "    from collections import defaultdict\n",
    "\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    import emot\n",
    "    import nltk\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "    from convokit import Corpus, PolitenessStrategies\n",
    "    from convokit.text_processing import TextProcessor, TextCleaner, TextParser\n",
    "    import spacy\n",
    "    import contractions as cm\n",
    "    from textblob import TextBlob as tb\n",
    "\n",
    "    from ...config import config\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "\n",
    "def def_value():\n",
    "    return 0\n",
    "\n",
    "\n",
    "def subjective_words():\n",
    "    subjectivity_clues = []\n",
    "    with open(os.path.join(config.ROOT_DIR, \"data\", \"external\", \"subjectivity_clues\"), \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            values = line.split(\" \")[2]\n",
    "            subjectivity_clues = values.split(\"=\")[1]\n",
    "            subjectivity_clues.append(subjectivity_clues)\n",
    "    return subjectivity_clues\n",
    "\n",
    "\n",
    "def clean(\n",
    "        text, newline=True, quote=True,\n",
    "        bullet_point=True, dates=True, link=True,\n",
    "        strikethrough=True, spoiler=True, heading=True,\n",
    "        emoj=True, emoticon=True, condensed=True):\n",
    "    # Newlines we don't need - only\n",
    "    \"\"\" Cleans reddit utterances\"\"\"\n",
    "\n",
    "    if newline:\n",
    "        text = re.sub(r'\\n+', ' ', text)\n",
    "        # Remove the many \" \" that we replaced in the last step\n",
    "        text = text.strip()\n",
    "        text = re.sub(r'\\s\\s+', ' ', text)\n",
    "\n",
    "    # > are for the quoted texts from the main comment or the reply\n",
    "    if quote:\n",
    "        text = re.sub(r'>', '', text)\n",
    "\n",
    "    # Bullet points/asterisk are used for markdown like - bold/italic - Could create trouble in parsing? idk\n",
    "    if bullet_point:\n",
    "        text = re.sub(r'\\*', '', text)\n",
    "        text = re.sub('&amp;#x200B;', '', text)\n",
    "\n",
    "    # []() Link format then we remove both the tag/placeholder and the link\n",
    "    if link:\n",
    "        text = re.sub(r\"http\\S+\", '', text)\n",
    "        text = re.sub(r'\\[.*?\\]\\(.*?\\)', '', text)\n",
    "\n",
    "    # Strikethrough\n",
    "    if strikethrough:\n",
    "        text = re.sub('~', '', text)\n",
    "\n",
    "    # Spoiler, which is used with < less-than (Preserves the text)\n",
    "    if spoiler:\n",
    "        text = re.sub('&lt;', '', text)\n",
    "        text = re.sub(r'!(.*?)!', r'\\1', text)\n",
    "\n",
    "    # Heading to be removed as there are these markdown style features in reddit too\n",
    "    if heading:\n",
    "        text = re.sub('#', '', text)\n",
    "\n",
    "    if emoj:\n",
    "        # Implement the emoji scheme here\n",
    "        # Makes more sense for the node feature but might as well import that function here if ready\n",
    "        # Implementing a Naive Emoji Scheme\n",
    "        # Some associated libraries are EMOT and DEMOJI\n",
    "        # text = emoji.demojize(text).replace(\":\", \"\").replace(\"_\", \"\")\n",
    "        pass\n",
    "\n",
    "    if dates:\n",
    "        text = re.sub(r'(\\d+/\\d+/\\d+)', '', text)\n",
    "\n",
    "    if emoticon:\n",
    "        # Implement the emoticon scheme here.\n",
    "        # Makes more sense for the node feature but might as well import that function here if ready\n",
    "        pass\n",
    "\n",
    "    # Needs to be the last step in the process\n",
    "    if condensed:\n",
    "        text = cm.fix(text)\n",
    "        # print(\"Running\")\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def convert_to_lower(utt: str) -> str:\n",
    "    \"\"\" This function block performs twitter text normalization\n",
    "\n",
    "        For instance, the different forms of 'hate' are: Hate, HATE, haTE, etc. This function would convert all such occurences to a single canonical form\n",
    "        \"\"\"\n",
    "    exclude_tags_list = ['NN', 'NNS', 'NNP', 'NNPS']  # Check if the attached POS tags are correct or not\n",
    "    sents = nltk.sent_tokenize(utt)\n",
    "    modified_sent_ls = []\n",
    "\n",
    "    for sent in sents:\n",
    "        modified_token_ls = []\n",
    "        words = nltk.word_tokenize(sent)  # Tokenize the sentence and extract POS tags\n",
    "        lemmatizer = WordNetLemmatizer()  # Performs Lemmatization\n",
    "        words = [lemmatizer.lemmatize(word) for word in words]  # Perform lemmatization if required\n",
    "        word_pos_tags = nltk.pos_tag(words)\n",
    "\n",
    "        for (word, tag) in word_pos_tags:\n",
    "            if tag not in exclude_tags_list or word != \"I\":\n",
    "                word = word.lower()\n",
    "                modified_token_ls.append(word)\n",
    "\n",
    "        modified_token_ls[0] = modified_token_ls[0].capitalize()\n",
    "\n",
    "        utt = \" \".join(modified_token_ls)\n",
    "        utt = utt.strip()\n",
    "        modified_sent_ls.append(sent)\n",
    "\n",
    "    final_text = \" \".join(modified_sent_ls)\n",
    "\n",
    "    return final_text\n",
    "\n",
    "\n",
    "# Use a combination of IDENTITY ATTACK and INSULT parameters to separate MICROAGGRESSIONS from OTHER HATE-SPEECH forms\n",
    "def sentiment_analyzer(utt: str) -> dict:\n",
    "    sentiment = SentimentIntensityAnalyzer()  # Intialize Vader Sentiment Analyzer\n",
    "    sentence_ls = nltk.sent_tokenize(utt)\n",
    "    sentiment_score_ls = []\n",
    "\n",
    "    for sent in sentence_ls:\n",
    "        sentiment_score_ls.append(sentiment.polarity_scores(sent))\n",
    "\n",
    "    pos_score_sum, neu_score_sum, neg_score_sum = 0, 0, 0\n",
    "    for sentiment_scores in sentiment_score_ls:\n",
    "        pos_score_sum += sentiment_scores[\"pos\"]\n",
    "        neu_score_sum += sentiment_scores[\"neu\"]\n",
    "        neg_score_sum += sentiment_scores[\"neg\"]\n",
    "\n",
    "    pos_score_sum_avg = round((pos_score_sum / len(sentiment_score_ls)), 3)\n",
    "    neu_score_sum_avg = round((neu_score_sum / len(sentiment_score_ls)), 3)\n",
    "    neg_score_sum_avg = round((neg_score_sum / len(sentiment_score_ls)), 3)\n",
    "\n",
    "    compound_sentiment_scores = {\n",
    "        \"pos\": pos_score_sum_avg,\n",
    "        \"neu\": neu_score_sum_avg,\n",
    "        \"neg\": neg_score_sum_avg\n",
    "    }\n",
    "    return compound_sentiment_scores\n",
    "\n",
    "def modifier_count(utt: str) -> int:  # Calculating less of something isn't always the best indicator. Instead the prevalence of something more than ususal is a better marker. # Optional - Emergency Toolkit\n",
    "    \"\"\"Count modifiers, i.e., adjectives and adverbs in an utterance\n",
    "    Practically every sentence has modifiers. This function doesn't act as a filter. It is intended to be applied to the entire dataframe\n",
    "    The function block can detect probable deceptive clues in tweets and reddit posts\n",
    "    Less usage of descriptive modifiers is a possible clue that the speaker is uncertain in his claims/opinions.\n",
    "    \"\"\"\n",
    "\n",
    "    adj_pos_tags = ['JJ', 'JJR', 'JJS']  # POS tags describing adjectives\n",
    "    adv_pos_tags = ['RB', 'RBR', 'RBS']  # POS tags for adverbs\n",
    "    words = nltk.word_tokenize(utt)\n",
    "    word_tag_lst = nltk.pos_tag(words)\n",
    "    mod_count_dict = defaultdict(def_value)\n",
    "    count_mod_tags = 0\n",
    "    for (word, tag) in word_tag_lst:\n",
    "        if tag in adj_pos_tags or tag in adv_pos_tags:\n",
    "            count_mod_tags += 1\n",
    "            mod_count_dict[word] += 1\n",
    "    return {\"modifier_count_dict\": mod_count_dict, \"count_mod_tags\": count_mod_tags}\n",
    "\n",
    "\n",
    "def hedge_count(utt: str) -> int:\n",
    "    \"\"\" Count the list of all modal verbs that indicate possibility, but not certainty\n",
    "    The function block can detect probable deceptive clues in tweets and reddit posts\n",
    "    More usage of uncertain modal verbs is a possible clue that the speaker is uncertain in his utterance\n",
    "    \"\"\"\n",
    "    count_hedges, hedge_count_dict = 0, defaultdict(def_value)\n",
    "    set_of_hedges_en = [\"almost\", \"apparent\", \"apparently\", \"appear\", \"appeared\", \"appears\", \"approximately\", \"argue\",\n",
    "                        \"argued\", \"argues\", \"around\", \"assume\", \"assumed\", \"broadly\", \"certain amount\",\n",
    "                        \"certain extent\", \"certain level\", \"claim\", \"claimed\", \"claims\", \"doubt\", \"doubtful\",\n",
    "                        \"essentially\", \"estimate\", \"estimated\", \"fairly\", \"feel\", \"feels\", \"felt\", \"frequently\",\n",
    "                        \"from my perspective\",\n",
    "                        \"from our perspective\", \"from this perspective\", \"generally\", \"guess\", \"in general\",\n",
    "                        \"in most cases\", \"in most instances\", \"in my opinion\", \"in my view\", \"in our opinion\",\n",
    "                        \"in our view\",\n",
    "                        \"in this view\", \"indicate\", \"indicated\", \"indicates\", \"largely\", \"likely\", \"mainly\", \"may\",\n",
    "                        \"maybe\", \"might\", \"mostly\", \"often\", \"on the whole\", \"ought\", \"perhaps\", \"plausible\",\n",
    "                        \"plausibly\", \"possible\",\n",
    "                        \"possibly\", \"postulate\", \"postulated\", \"postulates\", \"presumable\", \"presumably\", \"probable\",\n",
    "                        \"probably\", \"quite\", \"rather\", \"relatively\", \"roughly\", \"seems\", \"should\", \"sometimes\",\n",
    "                        \"somewhat\", \"suggest\",\n",
    "                        \"suggested\", \"suggests\", \"suppose\", \"supposed\", \"supposes\", \"suspect\", \"suspects\", \"tend to\",\n",
    "                        \"tended to\", \"tends to\", \"think\", \"thinking\", \"thought\", \"to my knowledge\", \"typical\",\n",
    "                        \"typically\", \"uncertain\",\n",
    "                        \"uncertainly\", \"unclear\", \"unclearly\", \"unlikely\",\n",
    "                        \"usually\"]  # The Hedge word list has been taken from \"https://github.com/tslmy/politeness-estimator.git\"\n",
    "\n",
    "    pos_modal_ls = [\"shall\", \"should\", \"can\", \"could\", \"will\", \"would\", \"may\", \"must\",\n",
    "                    \"might\"]  # List of 9 modal verbs indicating possibility\n",
    "\n",
    "    hedges_modals = set_of_hedges_en + pos_modal_ls\n",
    "\n",
    "    words = utt.lower().split(\" \")\n",
    "    for word in words:\n",
    "        if word in hedges_modals:\n",
    "            count_hedges += 1\n",
    "            hedge_count_dict[word] += 1\n",
    "    return {\"hedge_count_dict\": hedge_count_dict, \"count_hedges\": count_hedges}\n",
    "\n",
    "\n",
    "def group_ref_count(utt: str) -> int:  # Include all third-party pronouns as well\n",
    "    \"\"\"Count list of group references\n",
    "    Usage of more self-references along with subjectivity score is a possible indication of deception\n",
    "    \"\"\"\n",
    "    count_group_ref, group_ref_count_dict = 0, defaultdict(def_value)\n",
    "    words = utt.lower().split()\n",
    "    group_ref = [\"we\", \"our\", \"ours\", \"ourselves\", \"us\", \"they\", \"them\", \"thesmselves\", \"their\", \"theirs\",\n",
    "                 \"everyone\", \"everybody\"]  # More of it to be included here. Self-referencing pronouns\n",
    "\n",
    "    for word in words:\n",
    "        if word in group_ref:\n",
    "            count_group_ref += 1\n",
    "            group_ref_count_dict[word] += 1\n",
    "    return {\"group_ref_count_dict\": group_ref_count_dict, \"count_group_ref\": count_group_ref}\n",
    "\n",
    "\n",
    "def subjectivity_utterance(utt: str) -> int:\n",
    "    \"\"\" Textblob subjectivity score\n",
    "    A higher subjective score indicates personal opinion.\n",
    "    Low subjective scores could be a possible indicator of deception. To be used along with self references.\n",
    "    \"\"\"\n",
    "    subjective_lexicon_count, subjective_lexicon_dict, subjectivity_scores = 0, defaultdict(def_value), []\n",
    "    sents = nltk.sent_tokenize(utt)\n",
    "\n",
    "    for sent in sents:\n",
    "        subjectivity_scores.append(tb(sent).sentiment.subjectivity)\n",
    "        words = sent.lower().split()\n",
    "\n",
    "        for word in words:\n",
    "            subjectivity_clues = subjective_words()\n",
    "            if word in subjectivity_clues:\n",
    "                subjective_lexicon_dict[word] += 1\n",
    "                subjective_lexicon_count += 1\n",
    "\n",
    "    avg_subjectivity_score = round((sum(subjectivity_scores) / len(subjectivity_scores)), 3)\n",
    "    subjective_details = {\n",
    "        \"avg_subjectivity_score\": avg_subjectivity_score,\n",
    "        \"subjective_lexicon_count\": subjective_lexicon_count,\n",
    "        \"subjective_lexicon_dict\": subjective_lexicon_dict\n",
    "    }\n",
    "\n",
    "    return subjective_details\n",
    "\n",
    "\n",
    "def measurePoliteness(utt: str):\n",
    "    \"\"\"\n",
    "    Computes politeness indicators in the text. The 9 positive politeness strategies\n",
    "    \"\"\"\n",
    "    politeness = PolitenessStrategies()  # Politeness Indicators\n",
    "    spacy_nlp = spacy.load('en_core_web_md', disable=[\"ner\"])  # Spacy Language Model\n",
    "    transformed_utt = politeness.transform_utterance(utt, spacy_nlp=spacy_nlp)\n",
    "    return transformed_utt.meta['politeness_strategies']\n",
    "\n",
    "\n",
    "def count_emojis(utt: str) -> int:  # Optional - Emergency Toolkit -\n",
    "    \"\"\" Counts the total number of emojis in an utterance\n",
    "    We don't intend to delete tweets that have emojis. This function doesn't act as a filter. It is intended to be applied to the entire dataframe\n",
    "    Decide if it is redundant or not - maybe some possible indicators - not the first choice anyhow\n",
    "    \"\"\"\n",
    "    emotion = emot.core.emot()  # Initialize Emoji Object\n",
    "    emot_dict = emotion.emoji(utt)\n",
    "\n",
    "    return len(emot_dict['value'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a241c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load C:\\Sagar Study\\ML and Learning\\CP Sem-8\\Project-2-Overall\\nlu-project-prerocessing\\nlu-project-data\\src\\features\\pipeline.py\n",
    "from convokit.text_processing import TextProcessor, TextCleaner\n",
    "from convokit.convokitPipeline import ConvokitPipeline\n",
    "from heuristic_features import clean, convert_to_lower, sentiment_analyzer, modifier_count, \\\n",
    "    hedge_count\n",
    "from heuristic_features import group_ref_count, subjectivity_utterance, measurePoliteness\n",
    "\n",
    "\n",
    "def pipe():  # The Pipe would be expanded further\n",
    "    clean_reddit_text = TextCleaner(text_cleaner=clean, replace_text=False, save_original=True)\n",
    "    lowercase_text = TextProcessor(proc_fn=convert_to_lower, output_field=\"lowercase_text\")\n",
    "    analyze_sentiment = TextProcessor(proc_fn=sentiment_analyzer, input_field=\"lowercase_text\",\n",
    "                                      output_field=\"sentiment_polarity\")\n",
    "    subjectivity = TextProcessor(proc_fn=subjectivity_utterance, input_field=\"lowercase_text\",\n",
    "                                 output_field=\"subjectivity_score\")\n",
    "    modifier_count = TextProcessor(proc_fn=modifier_count, input_field=\"lowercase_text\",\n",
    "                                   output_field=\"modifier_count\")\n",
    "    hedge_modals = TextProcessor(proc_fn=hedge_count, input_field=\"lowercase_text\",\n",
    "                                 output_field=\"hedge_count\")\n",
    "    group_ref_count = TextProcessor(proc_fn=group_ref_count, input_field=\"lowercase_text\",\n",
    "                                    output_field=\"groupRef_count\")\n",
    "    politeness_markers = TextProcessor(proc_fn=measurePoliteness, input_field=\"lowercase_text\",\n",
    "                                       output_field=\"politeness_markers\")\n",
    "    metadata_pipe = ConvokitPipeline([\n",
    "        (\"convert to lowercase\", lowercase_text),\n",
    "        (\"sentiment analyzer\", analyze_sentiment),\n",
    "        (\"count modifiers\", modifier_count),\n",
    "        (\"count hedges and modals\", hedge_modals),\n",
    "        (\"count group references\", group_ref_count),\n",
    "        (\"subjectivity score\", subjectivity),\n",
    "        (\"politeness indicators\", politeness_markers)\n",
    "    ])\n",
    "\n",
    "    return metadata_pipe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d765a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load C:\\Sagar Study\\ML and Learning\\CP Sem-8\\Project-2-Overall\\nlu-project-prerocessing\\nlu-project-data\\src\\features\\perspective_hate_ma.py\n",
    "try:\n",
    "    import os\n",
    "    import json\n",
    "    from typing import Dict\n",
    "    from convokit import Utterance, Corpus\n",
    "    from googleapiclient import discovery\n",
    "    from tqdm import tqdm\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "\n",
    "def hate_microaggression_polarity(utt: Utterance) -> Dict:\n",
    "    \"\"\"\n",
    "    Calculates probability scores for production attributes of Perspective API\n",
    "    A detailed description can be obtained at \"https://developers.perspectiveapi.com/s/about-the-api-attributes-and-languages\"\n",
    "\n",
    "\n",
    "    Args:\n",
    "        utt: Convokit utterance object\n",
    "\n",
    "    Returns:\n",
    "        Probability scores for the parameters\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    developer_key = os.environ.get(\"GOOGLE_API_CLIENT_INSTITUTION\")  # load up the entries as environment variables\n",
    "\n",
    "    client = discovery.build(  # Initialize the client\n",
    "        \"commentanalyzer\",\n",
    "        \"v1alpha1\",\n",
    "        developerKey=developer_key,\n",
    "        discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
    "    )\n",
    "\n",
    "    analyze_request = {\n",
    "        'comment': {'text': utt},\n",
    "        'requestedAttributes': {\n",
    "            'TOXICITY': {},\n",
    "            'SEVERE_TOXICITY': {},\n",
    "            'IDENTITY_ATTACK': {},\n",
    "            'INSULT': {},\n",
    "            'PROFANITY': {}\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = client.comments().analyze(body=analyze_request).execute()\n",
    "    response_dict = dict(response)\n",
    "\n",
    "    attributes = [\"toxicity\", \"severe_toxicity\", \"identity_attack\", \"insult\", \"profanity\"]\n",
    "    attribute_values = {}\n",
    "\n",
    "    for attr in attributes:\n",
    "        attribute_values[attr] = response_dict[\"attributeScores\"][attr][\"spanScores\"][0][\"score\"][\"value\"]\n",
    "\n",
    "    return attribute_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414e77e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Reddit Corpus \n",
    "try:\n",
    "    CORPUS_PATH = \"fc(1-1)_30p_1\"   # Provide the path \n",
    "    reddit_corpus = Corpus(CORPUS_PATH)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d8a3cc08",
   "metadata": {
    "id": "d8a3cc08",
    "outputId": "cc4116a1-2494-45ec-9514-4213051ac7c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congratulations, the reddit corpus has been transformed\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Transform the reddit corpus \n",
    "    transformed_corpus_reddit = pipe_reddit.transform(reddit_corpus, verbosity = 50)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "else:\n",
    "    print(\"Congratulations, the reddit corpus has been transformed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec61057",
   "metadata": {
    "id": "0ec61057"
   },
   "source": [
    "## Dump the corpus "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f878c0",
   "metadata": {
    "id": "93f878c0"
   },
   "source": [
    "By default, calling **corpus.dump** will write all of these attributes to disk, within the file that stores utterances; later calling **corpus.load** will load all of these attributes back into a new corpus. For big objects like parses, this incurs a high computational burden (especially if in a later use case you might not even need to look at parses).\n",
    "\n",
    "To avoid this, **corpus.dump** takes an optional argument **fields_to_skip**, which is a **dict of object type ('utterance', 'conversation', 'speaker', 'corpus')** to a list of fields that we do not want to write to disk.\n",
    "\n",
    "The following call will write the corpus to disk, without any of the preprocessing output we generated above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "173b6929",
   "metadata": {
    "id": "173b6929",
    "outputId": "d83ee9c5-98c3-4210-e70a-3fa2f0fbfda5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO OUTPUT DIRECTORY\n"
     ]
    }
   ],
   "source": [
    "# Demonstration of re-dumping of the transformed corpus after first batch of test\n",
    "try:\n",
    "    corpus.dump(os.path.basename(OUT_DIR), base_path=os.path.dirname(OUT_DIR), \n",
    "            fields_to_skip={'utterance': ['parsed','tagged','clean_text']})    # Just random field names \n",
    "    \n",
    "except:\n",
    "    print(\"NO OUTPUT DIRECTORY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d749f9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration of re-dumping of the transformed corpus after first batch of test\n",
    "try:\n",
    "    CORPUS_NAME = \"tr_fc(12-23)_27p_1\"\n",
    "    OUTPUT_DIR = \"C:\\Sagar Study\\ML and Learning\\CP Sem-8\\Data\\Reddit\\saved-corpora\\sanskar_transformed_corpora\"\n",
    "    transformed_corpus_reddit.dump(CORPUS_NAME, base_path=OUTPUT_DIR)\n",
    "    \n",
    "except:\n",
    "    print(\"NO OUTPUT DIRECTORY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39b5bc2",
   "metadata": {
    "id": "e39b5bc2"
   },
   "source": [
    "For attributes we want to keep around, but that we don't want to read and write to disk in a big batch with all the other corpus data, corpus.dump_info will dump fields of a Corpus object into separate files. This takes the following arguments as input:\n",
    "\n",
    "* obj_type: which type of Corpus object you're dealing with\n",
    "* fields: a list of the fields to write\n",
    "* dir_name: which directory to write to; by default will write to the directory you read the corpus from\n",
    "\n",
    "This function will write each field in fields to a separate file called **info.<field>.jsonl** where each line of the file is a json-serialized dict: {\"id\": <ID of object>, \"value\": <object.get_info(field)>}."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "2.0_function_migration_pipeline.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
